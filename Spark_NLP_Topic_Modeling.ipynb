{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "luu97BMUVWOZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1cwEJpyf7gAfTmlVS8Jd8yiA1NyfO7wOG\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL0YKjTw8VNt"
      },
      "source": [
        "## **Goal: Quickly get a sense of prevalent topics discussed in hundred of thousands of reader comments of NY Times articles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLuSUt68kgR"
      },
      "source": [
        "**Source data** from here: https://www.kaggle.com/datasets/aashita/nyt-comments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rt_Wn6u5movh"
      },
      "source": [
        "**Import PySpark and Spark NLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kN5G8L86lvkd"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyspark==3.3.0 spark-nlp==4.2.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCRa7NCem0IK"
      },
      "source": [
        "**Starting of Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jp8CJ1Ozlygo"
      },
      "outputs": [],
      "source": [
        "from sparknlp.base import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import PretrainedPipeline\n",
        "import sparknlp\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "spark = sparknlp.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueGT2Pg_8xAF"
      },
      "source": [
        "**Upload sample CSV file to Colab session storage and read into Spark dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTc_ogobm9YZ",
        "outputId": "ad374ccd-a3af-484d-d2db-383cdbb0a709"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "243866"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = spark.read.format(\"csv\").option(\"header\",True).load(\"/content/CommentsApril2017.csv\")\n",
        "df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nvRYf4fjn_k0",
        "outputId": "9dc6cd57-b6b0-4b2c-e887-0d9f53dc7e68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- approveDate: string (nullable = true)\n",
            " |-- commentBody: string (nullable = true)\n",
            " |-- commentID: string (nullable = true)\n",
            " |-- commentSequence: string (nullable = true)\n",
            " |-- commentTitle: string (nullable = true)\n",
            " |-- commentType: string (nullable = true)\n",
            " |-- createDate: string (nullable = true)\n",
            " |-- depth: string (nullable = true)\n",
            " |-- editorsSelection: string (nullable = true)\n",
            " |-- parentID: string (nullable = true)\n",
            " |-- parentUserDisplayName: string (nullable = true)\n",
            " |-- permID: string (nullable = true)\n",
            " |-- picURL: string (nullable = true)\n",
            " |-- recommendations: string (nullable = true)\n",
            " |-- recommendedFlag: string (nullable = true)\n",
            " |-- replyCount: string (nullable = true)\n",
            " |-- reportAbuseFlag: string (nullable = true)\n",
            " |-- sharing: string (nullable = true)\n",
            " |-- status: string (nullable = true)\n",
            " |-- timespeople: string (nullable = true)\n",
            " |-- trusted: string (nullable = true)\n",
            " |-- updateDate: string (nullable = true)\n",
            " |-- userDisplayName: string (nullable = true)\n",
            " |-- userID: string (nullable = true)\n",
            " |-- userLocation: string (nullable = true)\n",
            " |-- userTitle: string (nullable = true)\n",
            " |-- userURL: string (nullable = true)\n",
            " |-- inReplyTo: string (nullable = true)\n",
            " |-- articleID: string (nullable = true)\n",
            " |-- sectionName: string (nullable = true)\n",
            " |-- newDesk: string (nullable = true)\n",
            " |-- articleWordCount: string (nullable = true)\n",
            " |-- printPage: string (nullable = true)\n",
            " |-- typeOfMaterial: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tE7vwarAoWs8"
      },
      "source": [
        "**Pre-processing Pipeline using Spark NLP**<br>\n",
        "See Spark NLP pipelines concept here https://nlp.johnsnowlabs.com/api/python/user_guide/custom_pipelines.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zFMRAOrIoeTL"
      },
      "outputs": [],
      "source": [
        "# Spark NLP requires the input dataframe or column to be converted to document\n",
        "document_assembler = DocumentAssembler() \\\n",
        "    .setInputCol(\"commentBody\") \\\n",
        "    .setOutputCol(\"document\") \\\n",
        "    .setCleanupMode(\"shrink\")\n",
        "\n",
        "# Split sentence into tokens\n",
        "tokenizer = Tokenizer() \\\n",
        "  .setInputCols([\"document\"]) \\\n",
        "  .setOutputCol(\"token\")\n",
        "\n",
        "# Clean unwanted characters and garbage\n",
        "normalizer = Normalizer() \\\n",
        "    .setInputCols([\"token\"]) \\\n",
        "    .setOutputCol(\"normalized\")\n",
        "\n",
        "# Remove stopwords\n",
        "stopwords_cleaner = StopWordsCleaner()\\\n",
        "      .setInputCols(\"normalized\")\\\n",
        "      .setOutputCol(\"cleanTokens\")\\\n",
        "      .setCaseSensitive(False)\n",
        "\n",
        "finisher = Finisher() \\\n",
        "    .setInputCols([\"cleanTokens\"]) \\\n",
        "    .setOutputCols([\"tokens\"]) \\\n",
        "    .setOutputAsArray(True) \\\n",
        "    .setCleanAnnotations(False)\n",
        "\n",
        "# Build pipeline\n",
        "nlp_pipeline = Pipeline(\n",
        "    stages=[document_assembler, \n",
        "            tokenizer,\n",
        "            normalizer,\n",
        "            stopwords_cleaner, \n",
        "            finisher])\n",
        "\n",
        "# Train pipeline\n",
        "nlp_model = nlp_pipeline.fit(df)\n",
        "\n",
        "# Apply pipeline to dataframe\n",
        "processed_df  = nlp_model.transform(df)\n",
        "\n",
        "# Get tokens\n",
        "tokens_df = processed_df.select('tokens').limit(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG-UVD_Xovy0"
      },
      "source": [
        "**Feature Engineering**<br>\n",
        "See CountVectorizer docs here https://spark.apache.org/docs/latest/ml-features#countvectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TYOU4Ttpo1QF"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=1000, minDF=3.0)\n",
        "\n",
        "# Train the model\n",
        "cv_model = cv.fit(tokens_df)\n",
        "\n",
        "# Transform the data and output features\n",
        "vectorized_tokens = cv_model.transform(tokens_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGtOWP9LpErf"
      },
      "source": [
        "**Build the LDA (Latent Dirichlet Allocation) Model**<br>\n",
        "See LDA docs here https://spark.apache.org/docs/3.3.2/ml-clustering.html#latent-dirichlet-allocation-lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1mr_8zJmpQ7Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.clustering import LDA\n",
        "\n",
        "num_topics = 15\n",
        "lda = LDA(k=num_topics, maxIter=10)\n",
        "model = lda.fit(vectorized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwIaESFuqUKN"
      },
      "source": [
        "**Visualize the topics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xurvcp6NqeJL",
        "outputId": "0f00fe99-df7f-47bb-c534-6d9b7a0560fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------------------------------------------------------------------+\n",
            "|topic|words                                                                        |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "|0    |[money, people, see, million, facts, real, time, lot, like, face]            |\n",
            "|1    |[white, black, Venezuela, Im, people, food, called, never, People, identity] |\n",
            "|2    |[Trump, think, must, much, voters, Bill, people, Democrats, OReilly, vote]   |\n",
            "|3    |[full, stories, get, always, Clinton, new, way, given, Uber, people]         |\n",
            "|4    |[man, people, like, candy, cotton, looks, picture, selling, graveyard, think]|\n",
            "|5    |[Sean, Spicer, catch, children, playing, time, baseball, God, school, work]  |\n",
            "|6    |[Trump, Trumps, President, people, hate, one, House, American, brain, White] |\n",
            "|7    |[show, season, one, get, puzzle, first, never, around, mice, still]          |\n",
            "|8    |[people, one, many, said, family, long, like, get, political, Jewish]        |\n",
            "|9    |[time, NCAA, play, years, sports, players, college, game, team, playing]     |\n",
            "|10   |[Trump, people, one, Fox, many, Vancouver, doesnt, also, world, supporters]  |\n",
            "|11   |[women, men, sexual, harassment, case, article, Uber, OReilly, still, sex]   |\n",
            "|12   |[money, Uber, lose, culture, sentence, seem, given, job, general, run]       |\n",
            "|13   |[like, people, dont, think, Trump, get, us, going, way, government]          |\n",
            "|14   |[tax, pay, border, US, jobs, companies, prices, cost, income, goods]         |\n",
            "+-----+-----------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Extract vocabulary from CountVectorizer\n",
        "vocab = cv_model.vocabulary\n",
        "topics = model.describeTopics()   \n",
        "topics_rdd = topics.rdd\n",
        "topics_words = topics_rdd \\\n",
        "       .map(lambda row: row['termIndices']) \\\n",
        "       .map(lambda idx_list: [vocab[idx] for idx in idx_list]) \\\n",
        "       .collect()\n",
        "\n",
        "# Add each array of words to a new column\n",
        "def get_words(idx_list):\n",
        "    return [vocab[idx] for idx in idx_list]\n",
        "\n",
        "udf_get_words = udf(get_words, ArrayType(StringType()))\n",
        "topics = topics.withColumn(\"words\", udf_get_words(topics.termIndices))\n",
        "\n",
        "# From topics dataframa select topic and words columns\n",
        "topics_df = topics.select(\"topic\", \"words\")\n",
        "\n",
        "topics_df.show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
